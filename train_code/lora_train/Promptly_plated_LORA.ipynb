{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lTGoZ0G2fI4M","outputId":"98b8ae10-2781-432c-94cc-0e6f81d0135d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.24.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.17.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"]}],"source":["# https://blog.accubits.com/how-to-fine-tune-llama-2-using-sft-lora/\n","\n","!pip install transformers\n","!pip install accelerate\n","!pip install fire"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MSwtxkrEgfUN"},"outputs":[],"source":["!pip install datasets\n","!pip install peft\n","!pip install sentencepiece"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xKU30ZfxwNiC"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rgyhkKRfwsQ8"},"outputs":[],"source":["output_directory = '/content/drive/Othercomputers/My MacBook Air/train_code/lora_train/'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OY8xhDIlxFPl"},"outputs":[],"source":["# import os\n","# if not os.path.exists(output_directory):\n","#     os.makedirs(output_directory)\n","\n","# # Now you can save files to 'output_dir'\n","# test_file_path = os.path.join(output_directory, 'test_file.txt')\n","# with open(test_file_path, 'w') as file:\n","#     file.write('This is a test file.')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g5gKEgXQUfXK"},"outputs":[],"source":["import transformers\n","import fire\n","import torch\n","\n","from datasets import load_dataset\n","from torch import cuda\n","from peft import (\n","    LoraConfig,\n","    get_peft_model,\n","    get_peft_model_state_dict,\n","    prepare_model_for_int8_training,\n","    set_peft_model_state_dict,\n","    PrefixTuningConfig,\n","    TaskType\n",")\n","from transformers import LlamaForCausalLM, LlamaTokenizer\n","\n","from typing import List"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HkslvUtVjefp"},"outputs":[],"source":["from huggingface_hub import login\n","\n","access_token_read =\"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\"\n","\n","login(token = access_token_read)"]},{"cell_type":"markdown","metadata":{"id":"qm5OtLI4VJBS"},"source":["## Formatted Recipe Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cQ6Fyu6pVGu7"},"outputs":[],"source":["# dataset = load_dataset(\"m3hrdadfi/recipe_nlg_lite\")\n","# print(dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OVh-hoDD22iR"},"outputs":[],"source":["# print(dataset['train']['steps'][0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W7UTox4g3ahZ"},"outputs":[],"source":["# dataset = load_dataset(\"tatsu-lab/alpaca\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zN3KO_0I3f4Y"},"outputs":[],"source":["# print(len(dataset['train']['input']))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZZ9CQTVfVMhf"},"outputs":[],"source":["# import pandas as pd\n","# train_set = dataset[\"train\"]\n","# test_set = dataset[\"test\"]\n","\n","# rd_df = pd.DataFrame(dataset['train'])\n","# rd_df['instruction'] = 'Use these ingredients to generate a recipe: '+ rd_df['ingredients']\n","# rd_df['response'] = 'Title:\\n' + rd_df['name'] + '\\nSteps:\\n' + rd_df['steps']\n","\n","# template = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n","\n","# ### Instruction:\n","\n","# {}\n","\n","# ### Response:\\n\"\"\"\n","\n","# rd_df_sample['prompt'] = rd_df_sample[\"instruction\"].apply(lambda x: template.format(x))\n","# print(rd_df['response'][0])"]},{"cell_type":"markdown","metadata":{"id":"bd_Bk1oBUVGc"},"source":["## Train LoRA Layers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B3BsoLYZeQEG"},"outputs":[],"source":["def train(\n","    # model/data params\n","    base_model: str = \"meta-llama/Llama-2-7b-chat-hf\",\n","    data_path: str = \"m3hrdadfi/recipe_nlg_lite\",\n","    output_dir: str = output_directory,\n","\n","    micro_batch_size: int = 4,\n","    gradient_accumulation_steps: int = 4,\n","    num_epochs: int = 3,\n","    learning_rate: float = 3e-4,\n","    val_set_size: int = 2000,\n","\n","    # lora hyperparams\n","    lora_r: int = 8,\n","    lora_alpha: int = 16,\n","    lora_dropout: float = 0.05,\n","    lora_target_modules: List[str] = [\n","        \"q_proj\",\n","        \"v_proj\",\n","    ]\n","):\n","\n","    device_map = \"auto\"\n","\n","\n","    # Step 1: Load the model and tokenizer\n","\n","    model = LlamaForCausalLM.from_pretrained(\n","        base_model,\n","        # load_in_8bit=True, # Add this for using int8\n","        torch_dtype=torch.float16,\n","        device_map=device_map,\n","    )\n","\n","    tokenizer = LlamaTokenizer.from_pretrained(base_model)\n","    tokenizer.pad_token_id = 0\n","\n","    #Add this for training LoRA\n","\n","    config = LoraConfig(\n","        r=lora_r,\n","        lora_alpha=lora_alpha,\n","        target_modules=lora_target_modules,\n","        lora_dropout=lora_dropout,\n","        bias=\"none\",\n","        task_type=\"CAUSAL_LM\",\n","    )\n","\n","    model = get_peft_model(model, config)\n","\n","    model = prepare_model_for_int8_training(model) # Add this for using int8\n","\n","\n","    # Step 2: Load the data\n","\n","    if data_path.endswith(\".json\") or data_path.endswith(\".jsonl\"):\n","        data = load_dataset(\"json\", data_files=data_path)\n","    else:\n","        data = load_dataset(data_path)\n","\n","    # Step 3: Tokenize the data\n","\n","    def tokenize(data):\n","        source_ids = tokenizer.encode(data['ingredients'])\n","        target_ids = tokenizer.encode(data['steps'])\n","\n","        input_ids = source_ids + target_ids + [tokenizer.eos_token_id]\n","        labels = [-100] * len(source_ids) + target_ids + [tokenizer.eos_token_id]\n","\n","        return {\n","            \"input_ids\": input_ids,\n","            \"labels\": labels\n","        }\n","\n","    #split the data to train/val set\n","    train_val = data[\"train\"].train_test_split(\n","        test_size=val_set_size, shuffle=False, seed=42\n","    )\n","    train_data = (\n","        train_val[\"train\"].shuffle().map(tokenize)\n","    )\n","    val_data = (\n","        train_val[\"test\"].shuffle().map(tokenize)\n","\n","    )\n","\n","    # Step 4: Initiate the trainer\n","\n","    trainer = transformers.Trainer(\n","        model=model,\n","        train_dataset=train_data,\n","        eval_dataset=val_data,\n","        args=transformers.TrainingArguments(\n","            per_device_train_batch_size=micro_batch_size,\n","            gradient_accumulation_steps=gradient_accumulation_steps,\n","            warmup_steps=100,\n","            num_train_epochs=num_epochs,\n","            learning_rate=learning_rate,\n","            fp16=True,\n","            logging_steps=10,\n","            optim=\"adamw_torch\",\n","            evaluation_strategy=\"steps\",\n","            save_strategy=\"steps\",\n","            eval_steps=200,\n","            save_steps=200,\n","            output_dir=output_dir,\n","            save_total_limit=3\n","        ),\n","        data_collator=transformers.DataCollatorForSeq2Seq(\n","            tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n","        ),\n","    )\n","\n","    trainer.train()\n","\n","\n","    # Step 5: save the model\n","    model.save_pretrained(output_dir)\n","\n","\n","if __name__ == \"__main__\":\n","    fire.Fire(train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sC4bgdCcu0PL"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"gpuType":"A100"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}